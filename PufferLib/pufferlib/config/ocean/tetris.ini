[base]
package = ocean
env_name = puffer_tetris
policy_name = Policy
rnn_name = Recurrent

[vec]
num_envs = 8

[env]
num_envs = 2048
n_rows = 20
n_cols = 10
use_deck_obs = True
n_init_garbage = 4
# This is experimental. Sometimes it works.
n_noise_obs = 0

[policy]
hidden_size = 256

[rnn]
input_size = 256
hidden_size = 256

[train]
# https://wandb.ai/kywch/pufferlib/runs/era6a8p6?nw=nwuserkywch
total_timesteps = 3_000_000_000
batch_size = auto
bptt_horizon = 64
minibatch_size = 65536

adam_beta1 = 0.95
adam_beta2 = 0.9999
adam_eps = 1e-10
clip_coef = 0.1
ent_coef = 0.02
gae_lambda = 0.55
gamma = 0.995
learning_rate = 0.012
max_grad_norm = 5
prio_alpha = 0.99
prio_beta0 = 0.91
vf_clip_coef = 1.5
vf_coef = 4.74
vtrace_c_clip = 1.29
vtrace_rho_clip = 0.70


[sweep]
metric = score
goal = maximize

[sweep.train.total_timesteps]
distribution = log_normal
min = 30_000_000
max = 3_000_000_000
mean = 200_000_000
scale = auto

[sweep.train.gae_lambda]
distribution = logit_normal
min = 0.01
mean = 0.6
max = 0.995
scale = auto

[sweep.train.clip_coef]
distribution = uniform
min = 0.01
max = 1.0
mean = 0.1
scale = auto

[sweep.train.adam_beta1]
distribution = logit_normal
min = 0.5
mean = 0.95
max = 0.999
scale = auto
